# backend/main.py

import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from dotenv import load_dotenv
from openai import OpenAI

import chromadb
from chromadb.utils import embedding_functions

# ================== ê¸°ë³¸ ì„¤ì • ==================

BASE_DIR = Path(__file__).resolve().parent
RAG_DIR = BASE_DIR / "rag"
DB_DIR = RAG_DIR / "db"

COLLECTION_NAME = "yeobaek_docs"
EMBED_MODEL = "text-embedding-3-small"

load_dotenv()  # .env ì½ê¸°
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI(api_key=api_key)


# ================== Chroma / RAG(1119 ì‹ ê·œ) ==================

def get_rag_collection():
    """ì €ì¥ëœ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì»¬ë ‰ì…˜ ê°ì²´ë¥¼ ë¦¬í„´"""
    if not DB_DIR.exists():
        raise RuntimeError(
            f"RAG DB ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {DB_DIR} (rag/build_index.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”)"
        )

    chroma_client = chromadb.PersistentClient(path=str(DB_DIR))

    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=api_key,
        model_name=EMBED_MODEL,
    )

    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=openai_ef,
    )
    return collection


def make_context_str(documents: List[str], metadatas: List[dict]) -> str:
    """
    ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ í° context ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°.
    ê° ì²­í¬ ì•ì— [ì¶œì²˜: íŒŒì¼ëª…, chunk: n] ê¼¬ë¦¬í‘œë¥¼ ë¶™ì„.
    """
    blocks = []
    for doc, meta in zip(documents, metadatas):
        source = meta.get("source", "unknown")
        idx = meta.get("chunk_index", 0)
        header = f"[ì¶œì²˜: {source} | chunk #{idx}]"
        blocks.append(header + "\n" + doc.strip())
    return "\n\n---\n\n".join(blocks)


# ================== ë¡œê·¸ ì„¤ì • (ì˜µì…˜, 1119 ì‹ ê·œ) ==================

LOG_PATH = BASE_DIR / "logs" / "chat_logs.jsonl"
LOG_PATH.parent.mkdir(exist_ok=True)


def log_chat(user_message: str, reply: str, meta: Optional[dict] = None):
    """ëŒ€í™” í•œ ê±´ì„ jsonl í˜•ì‹ìœ¼ë¡œ ê¸°ë¡"""
    entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "user_message": user_message,
        "reply": reply,
        "meta": meta or {},
    }
    with LOG_PATH.open("a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")


# ================== FastAPI ì•± ==================

app = FastAPI(title="Yeobaek Chat Backend")

# CORS (í”„ë¡ íŠ¸: http://localhost:5173 ì—ì„œ ì ‘ê·¼, 1119 ì‹ ê·œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ================== ëª¨ë¸ ì •ì˜ ==================

class ChatRequest(BaseModel):
    message: str


class ChatResponse(BaseModel):
    reply: str


# ================== ì—”ë“œí¬ì¸íŠ¸ ==================

# ping test
@app.get("/api/ping")
def ping():
    return {"status": "ok"}


# 1) ì¼ë°˜ LLM Chat (RAG X)
@app.post("/api/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    try:
        completion = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a helpful assistant for Yeobaek club. "
                        "Answer briefly in Korean unless the user asks otherwise."
                    ),
                },
                {"role": "user", "content": req.message},
            ],
        )
        reply_text = completion.choices[0].message.content
        log_chat(
            user_message=req.message,
            reply=reply_text,
            meta={"model": "gpt-4.1-mini", "source": "openai", "mode": "plain"},
        )
        return ChatResponse(reply=reply_text)
    except Exception as e:
        print("[ERROR] /api/chat:", e)
        raise HTTPException(status_code=500, detail="OpenAI í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# 2) RAG + LLM Chat(1119 ì‹ ê·œ)
@app.post("/api/rag-chat", response_model=ChatResponse)
def rag_chat(req: ChatRequest):
    """
    1) ì‚¬ìš©ìì˜ ì§ˆë¬¸ìœ¼ë¡œ ChromaDBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
    2) ê²€ìƒ‰ëœ ë‚´ìš©(context)ì„ ë¶™ì—¬ì„œ OpenAIì—ê²Œ ì§ˆë¬¸
    3) Yeobaek/LIS ë„ë©”ì¸ ë‹µë³€ì„ ìƒì„±í•´ì„œ ë°˜í™˜
    """
    query = req.message.strip()
    if not query:
        raise HTTPException(status_code=400, detail="messageê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    try:
        collection = get_rag_collection()

        # 1) ë²¡í„° ê²€ìƒ‰ (Chromaê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì„ë² ë”© ê³„ì‚°)
        results = collection.query(
            query_texts=[query],
            n_results=4,  # ìƒìœ„ 4ê°œ ì²­í¬ ì‚¬ìš©
        )

        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]

        if not docs:
            # ë¬¸ì„œë¥¼ ëª» ì°¾ì€ ê²½ìš°: ì¼ë°˜ ë‹µë³€ìœ¼ë¡œ fallback
            fallback = (
                "ì•„ì§ ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.\n"
                "ê·¸ë˜ë„ ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìµœëŒ€í•œ ë‹µë³€í•´ë³¼ê²Œìš” ğŸ™‚"
            )
            context = ""
        else:
            context = make_context_str(docs, metas)
            fallback = None

        # 2) OpenAIì—ê²Œ context + ì§ˆë¬¸ ì „ë‹¬
        system_prompt = (
            "ë„ˆëŠ” ì¸ì²œëŒ€í•™êµ ë¬¸í—Œì •ë³´í•™ê³¼ ë™ì•„ë¦¬ 'ì—¬ë°±(Yeobaek)'ì˜ "
            "ì „ë¬¸ ë„ì„œê´€Â·ì •ë³´í•™(LIS) ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n"
            "- ì£¼ìš” ë„ë©”ì¸: ë¬¸í—Œì •ë³´í•™, ì •ë³´ê²€ìƒ‰(IR), ë©”íƒ€ë°ì´í„°/ëª©ë¡, ë¶„ë¥˜(KDC/DDC), "
            "ë””ì§€í„¸ ì•„ì¹´ì´ë¹™, ì¶”ì²œ ì‹œìŠ¤í…œ, ì‹œì†ŒëŸ¬ìŠ¤/ì˜¨í†¨ë¡œì§€, ë™ì•„ë¦¬ ìš´ì˜.\n"
            "- ì•„ë˜ ì œê³µëœ 'ì—¬ë°± í”„ë¡œì íŠ¸ ë¬¸ì„œ' ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´.\n"
            "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì•„ëŠ” ì²™í•˜ì§€ ë§ê³ , í™•ì‹¤íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•´.\n"
            "- ë‹µë³€ì€ í•œêµ­ì–´, 3~6ë¬¸ì¥ ì •ë„ë¡œ ê°„ê²°í•˜ê²Œ.\n"
            "- í•„ìš”í•˜ë©´ ì–´ë–¤ ë¬¸ì„œë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´."
        )

        user_content = query
        if context:
            user_content = (
                "ë‹¤ìŒì€ ì—¬ë°± í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì„œ ì¼ë¶€ì…ë‹ˆë‹¤:\n\n"
                f"{context}\n\n"
                "ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ, ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
                f"ì§ˆë¬¸: {query}"
            )

        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ],
        )

        reply_text = completion.choices[0].message.content

        # fallback ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì•ì— ì‚´ì§ ë¶™ì´ê¸°
        if fallback:
            reply_text = fallback + "\n\n" + reply_text

        # ë¡œê·¸ ê¸°ë¡
        log_chat(
            user_message=req.message,
            reply=reply_text,
            meta={
                "model": "gpt-4o-mini",
                "source": "openai",
                "mode": "rag",
                "docs_used": [m.get("source") for m in metas] if metas else [],
            },
        )

        return ChatResponse(reply=reply_text)

    except HTTPException:
        raise
    except Exception as e:
        print("[ERROR] /api/rag-chat:", e)
        raise HTTPException(
            status_code=500, detail="RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


# 3) ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (ì˜µì…˜)
@app.get("/api/logs/recent")
def get_recent_logs(limit: int = 20):
    """ìµœê·¼ ëŒ€í™” ë¡œê·¸ nê°œ ì¡°íšŒ (ê¸°ë³¸ 20ê°œ)"""
    if not LOG_PATH.exists():
        return []

    lines = LOG_PATH.read_text(encoding="utf-8").splitlines()
    data = [json.loads(line) for line in lines[-limit:]]
    return data